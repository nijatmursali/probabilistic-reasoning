{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hochschule Bonn-Rhein-Sieg\n",
    "\n",
    "# Probabilistic Methods for Robotics, WS21\n",
    "\n",
    "# Assignment 07\n",
    "\n",
    "Instructions for submission :\n",
    "- Please restart and run all cells before submitting \n",
    "- Make sure your user name is correct\n",
    "\n",
    "Good luck !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student user name: nmursa2s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: First- and second-order Markov process [20 points]\n",
    "\n",
    "Show that any second-order Markov process can be rewritten as a first-order Markov\n",
    "process with an augmented set of state variables. Can this always be done *parsimoniously*,\n",
    "i.e., without increasing the number of parameters needed to specify the transition model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that First-order Markov process says that current state depends only on the previous state and not on any earlier states\n",
    "\n",
    "$$ P(X_t | X_{0 : t-1}) = P(X_t | X_{t-1})$$\n",
    "\n",
    "Markov chains with higher order are the processes in which the next state depends on two or more preceding ones. So, in second-order we have the states depend on two preceding ones. \n",
    "\n",
    "There is nothing radically different about second order Markov chains: if $$P(x_i|x_{i-1},..,x_1)=P(x_i|x_{i-1},..,x_{i-n})$$ is a \"n-th order Markov chain\", it is also possible to write it as a first order Markov chain, on the space of combinations of $n$ states, For example, $S^n$, if $S$ is the combination/set of all values $x_i$ has: We can just write $$P(x_i|x_{i-1},..,x_1)=P(x_i|x_{i-1},..,x_{i-n})=P((x_i,x_{i-1},..,x_{i-n+1})|(x_{i-1},x_{i-2},..,x_{i-n}))$$\n",
    "\n",
    "Thus a second-order markov chain is just one which takes into account the two previous states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Speech recognition using the Viterbi algorithm [80 points]\n",
    "\n",
    "In assignment 1, we considered a simple speech synthesis system that generates sequences of the letters $\\{$ a, b, c, d $\\}$; now we are finally ready to examine that problem a little bit. In particular, we are going to start examining the Viterbi algorithm, which will be covered in the lectures in the following weeks.\n",
    "\n",
    "In assignment 1, we said that that the prior probabilities of generating the letters are $P(a) = 0.1$, $P(b) = 0.4$, $P(c) = 0.2$, and $P(d) = 0.3$; in addition, we assumed that we are given the following probabilities of generating letter $n+1$ given letter $n$:\n",
    "\n",
    "$P(a|a) = 0.2 \\hspace{2cm} P(b|a) = 0.1 \\hspace{2cm} P(c|a) = 0.6 \\hspace{2cm} P(d|a) = 0.1$\n",
    "\n",
    "$P(a|b) = 0.4 \\hspace{2cm} P(b|b) = 0.2 \\hspace{2cm} P(c|b) = 0.1 \\hspace{2cm} P(d|b) = 0.3$\n",
    "\n",
    "$P(a|c) = 0.1 \\hspace{2cm} P(b|c) = 0.2 \\hspace{2cm} P(c|c) = 0.4 \\hspace{2cm} P(d|c) = 0.3$\n",
    "\n",
    "$P(a|d) = 0.4 \\hspace{2cm} P(b|d) = 0.4 \\hspace{2cm} P(c|d) = 0.2 \\hspace{2cm} P(d|d) = 0.0$\n",
    "\n",
    "These are our *state transition probabilities*. We assume that a letter is correctly observed with $0.97$ probability; this means that there is $0.03$ probability that a letter will be observed incorrectly, a probability that is uniformly distributed over the other three letters. Given this information, we want to find answers to the following questions:\n",
    "\n",
    "1. Let's assume that we have observed the sequence $ac$; what is the most likely sequence that has generated this observation?\n",
    "2. We now observe the sequence $bbcdd$; what is the most likely sequence that has generated this observation?\n",
    "\n",
    "Your task is to implement the function *most_likely_sequence* in the cell below; this function should use the Viterbi algorithm for finding the most likely sequences that have generated the observed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely_sequence(observed_sequence: list, letters, priors, \\\n",
    "                         transition_probabilities, observation_probabilities):\n",
    "    max_sequence = list()\n",
    "    ### write your code here ###\n",
    "    total_states = len(observed_sequence)\n",
    "    viterbi = np.zeros((len(letters), len(observed_sequence)))\n",
    "    pointers = np.zeros((len(letters), len(observed_sequence)))\n",
    "    #viterbi = np.array([priors[i] * observation_probabilities[i] for i in range(0,len(letters))])\n",
    "    #pointers = np.array([priors[i] * observation_probabilities[i] for i in range(0,len(observed_sequence))])\n",
    "    # we need to fill values for observation 1 from the starting state s to number of states (in our case it is total_state)\n",
    "    for s in range(len(letters)): # we iterate through number of states \n",
    "        viterbi[s][0] = priors[s] * observation_probabilities[s][0]\n",
    "        pointers[s][0] = 0  \n",
    "    argmax = 0\n",
    "\n",
    "    print(viterbi)\n",
    "    print(viterbi.shape)\n",
    "    for o in range(1, total_states): # we iterate through t is observation time, \n",
    "        for s in range(len(letters)): # we iterate through s is our state\n",
    "            max = 0\n",
    "            argmax = 0\n",
    "            for i in range(1, total_states): # i iterates through number of all states \n",
    "                if max < viterbi[i][o - 1] * transition_probabilities[i][s] * observation_probabilities[s][o]:\n",
    "                    max = viterbi[i][o - 1] * transition_probabilities[i][s] * observation_probabilities[s][o]\n",
    "                    argmax = i\n",
    "            viterbi[s][o] = max # our trellis is equal to max\n",
    "            pointers[s][o] = argmax #pointers is equal to k which is argmax in our case\n",
    "\n",
    "    for i in range(0, total_states):\n",
    "        max_sequence.append(0)\n",
    "    max = 0\n",
    "    for i in range(1,total_states):\n",
    "        if max < viterbi[i][total_states- 1]:\n",
    "            max = viterbi[i][total_states - 1]\n",
    "            argmax = i\n",
    "            \n",
    "    max_sequence[total_states - 1] = argmax\n",
    "\n",
    "    for i in range(total_states - 1, 1, -1): ## Backtrack from last observation.\n",
    "        max_sequence[i - 1] = pointers[max_sequence[i]][i] ## Insert previous state on most likely path\n",
    "    for i in range(1, len(max_sequence)):\n",
    "        max_sequence.append(letters[max_sequence[i]])\n",
    "    ### your code ends here ###\n",
    "    return max_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your code\n",
    "\n",
    "The cell below describes the above problem and calls the *most_likely_sequence* function with the sequences $ac$ and $bbcdd$. Please run the cell once you're done with your implementation; you should obtain $ac$ and $bbcda$ as the most likely sequences corresponding to the observed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.097 0.   ]\n",
      " [0.004 0.   ]\n",
      " [0.002 0.   ]\n",
      " [0.003 0.   ]]\n",
      "(4, 2)\n",
      "['a', 'c']->[0, 1, 'b']\n",
      "[[0.097 0.    0.    0.    0.   ]\n",
      " [0.004 0.    0.    0.    0.   ]\n",
      " [0.002 0.    0.    0.    0.   ]\n",
      " [0.003 0.    0.    0.    0.   ]]\n",
      "(4, 5)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sq/cnmw1cts0hn9_9__lb3yxclm0000gn/T/ipykernel_33262/3848226059.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mobservation2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m max_sequence = most_likely_sequence(observation2, letters, priors, \\\n\u001b[0m\u001b[1;32m     21\u001b[0m                                     transition_probabilities, observation_probabilities)\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'->'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/sq/cnmw1cts0hn9_9__lb3yxclm0000gn/T/ipykernel_33262/2423672378.py\u001b[0m in \u001b[0;36mmost_likely_sequence\u001b[0;34m(observed_sequence, letters, priors, transition_probabilities, observation_probabilities)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0margmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# i iterates through number of all states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtransition_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mobservation_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtransition_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mobservation_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0margmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "###sample result\n",
    "#1)['a', 'c']->[]\n",
    "#2)['b', 'b', 'c', 'd', 'd']->[]\n",
    "\n",
    "letters = ['a', 'b', 'c', 'd']\n",
    "priors = np.array([0.1, 0.4, 0.2, 0.3])\n",
    "transition_probabilities = np.array([[0.2, 0.4, 0.1, 0.4],\\\n",
    "                                     [0.1, 0.2, 0.2, 0.4],\\\n",
    "                                     [0.6, 0.1, 0.4, 0.2],\\\n",
    "                                     [0.1, 0.3, 0.3, 0.0]])\n",
    "observation_probabilities = .01 * np.ones((4,4)) #so our observation probabilities are 4x4 array\n",
    "np.fill_diagonal(observation_probabilities, np.array([0.97, 0.97, 0.97, 0.97]))\n",
    "\n",
    "observation1 = ['a', 'c']\n",
    "max_sequence = most_likely_sequence(observation1, letters, priors, \\\n",
    "                                    transition_probabilities, observation_probabilities)\n",
    "print(str(observation1) +  '->' + str(max_sequence))\n",
    "\n",
    "observation2 = ['b', 'b', 'c', 'd', 'd']\n",
    "max_sequence = most_likely_sequence(observation2, letters, priors, \\\n",
    "                                    transition_probabilities, observation_probabilities)\n",
    "print(str(observation2) + '->' + str(max_sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
