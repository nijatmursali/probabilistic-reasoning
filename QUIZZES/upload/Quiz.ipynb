{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Hochschule Bonn-Rhein-Sieg\n",
    "\n",
    "# Probabilistic Reasoning, WS21\n",
    "\n",
    "# Quiz\n",
    "\n",
    "\n",
    "Instructions for submission :\n",
    "- Please restart and run all cells before submitting \n",
    "- Make sure your user name is correct\n",
    "- No need to submit a pdf file, only the ipython is sufficient\n",
    "- Write down theory questions in markdown format\n",
    "\n",
    "Good luck !!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Username : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1. Write down a short definition of the following terms.<br> (20 Points)\n",
    "\n",
    "**1. Probability theory**<br>\n",
    "\n",
    "\n",
    "**2. Utility theory**<br>\n",
    "\n",
    "\n",
    "**3. Decision theory**<br>\n",
    "\n",
    "\n",
    "**4. atomic event**\n",
    "\n",
    "\n",
    "**5. prior probability**\n",
    "\n",
    "\n",
    "**6. full joint probability distribution**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) For probability theory we assign each sentence to a probability between 0 and 1. So, all the probabilities should be between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) In probability every variable has a utility or usefulness and agent always tries to choose the higher utility. Utility theory is about it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Decision theory is the sum of utility theory and probability theory. Decision theory checks if agent is rational and agent is rational if it chooses the action with higher probability and utility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) It is complete specification of the world. As an example we can say toothache and cavity and in this case we assign a probability to both variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Prior probability is the probability of a variable associated with a and in this case prior probability will  be P(a). Prior proability is also called unconditional probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Let's give an example of P(cavity, toothache) from our slides. In this case our joint probability will be 2x2 because of values these two can take. Full joint probability distribution is a joint probability that covers the full set of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2. Associate each of the rules (1)-(5) with one of the equations (A)-(G) <br>(20 points)\n",
    "\n",
    "1. Chain Rule\n",
    "2. Conditioning rule\n",
    "3. Bayes' Rule\n",
    "4. Product Rule\n",
    "5. Marginalization Rule\n",
    "\n",
    "A. $   P(Y) = \\sum_z P(Y,z)  $ <br><br>\n",
    "B. $  P(X_i) = \\prod_{i=1}^{n} P(X_i|Y) P(Y) $ <br><br>\n",
    "C. $  P(Y) = \\sum_z P(Y|z)P(z) $ <br><br>\n",
    "D. $  P(x \\lor y) = P(x) + P(y) - P(x \\land y)$ <br><br>\n",
    "E. $  P(x_1,..., x_n) = \\prod_{i=1}^{n} P(x_i|x_{i-1},...,x_1) $ <br><br>\n",
    "F. $  P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)} $ <br><br>\n",
    "G. $  P(X,Y) = P(X|Y)P(Y) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### name of rule : Chain Rule\n",
    "#### probabilistic rule : E\n",
    "\n",
    "#### name of rule : Conditioning Rule\n",
    "#### probabilistic rule : C\n",
    "\n",
    "#### name of rule : Baye's Rule\n",
    "#### probabilistic rule : F\n",
    "\n",
    "#### name of rule : Product Rule\n",
    "#### probabilistic rule : G\n",
    "\n",
    "#### name of rule : Marginalization Rule\n",
    "#### probabilistic rule : A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3. Write down the axioms of probability (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. All the probabilities should be between 0 and 1, so 0<=P(a)<=1\n",
    "2. P(true)=1 and P(false)=0 which means event is true it is equal to 1 and false is equal to 0 \n",
    "3. Probability of conjunction is equal to P(a $\\vee$ b) = P(a) + P(b) - P(a $\\wedge$ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 4. Bayesian Networks (18 Points)\n",
    "\n",
    "1. Is the graph below a polytree?\n",
    "2. When is node F independent from node E?\n",
    "3. When is node D independent from node M?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "![title](bn_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Yes, it is polytree because as we know Bayesian networks are acyclic graph(no any loops) and polytrees are also acyclic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) F is conditionally independent of E when is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) D is conditionally independent from M when K is given. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 5. Inferencing in Bayesian Networks (24 Points)\n",
    "\n",
    "1. Write down the names of 2 exact inference algorithms for Bayesian networks and why are they not used in practice\n",
    "\n",
    "2. Write down the names of 3 approximate inference algorithms for Bayesian networks, and briefly describe how each of them works.\n",
    "\n",
    "3. In one of the approximate inference algorithms for Bayesian networks Markov blanket plays an important role. What is this algorithm  ? , and briefly explain why is it important in that algorithm ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Variable Elimination and Inference by enumerating. They are not really used because of the memory and time complexity due to having too many connected states. Variable Elimination solves it somehow as it eliminates some leaves (nodes) of the tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Rejection sampling, MCMC, Likelihood Weighting and Direct sampling methods. Rejection sampling is used by generating samples and rejecting the evidences that are not necessary. MCMC is called Markov Chain Monte Carlo and in this algorithm we also generate samples by making random changes. Likelihood weighting is another approximate inference algorithm and this is similar to rejection sampling, but in LW we generate samples that are consistent with evidences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) It is MCMC algorithm which is approximate inferencing algorithm and in this algorithm we generate a sample by making random changes. Markov blanket is used in MCMC and the main role is that as we generate new events sampling randomly the evidence variables which are generated by Markov blanket. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
